# ğŸ­ Day 2: Multimodal 3D Animation Integration Plan

## ğŸ“‹ **Context from Yesterday (Day 1)**

**Project**: Ani - AI Voice Companion
**Repository**: https://github.com/LookJohnny/AIWaifu
**Location**: F:\Ani\
**Current Server**: http://localhost:8000

### **What's Already Working** âœ…
- âœ… Bilingual AI (Chinese + English) with qwen2.5:7b
- âœ… Voice cloning with XTTS-v2 (wzy.wav custom voice)
- âœ… Real-time voice conversation loop
- âœ… Professional web UI with status indicators
- âœ… Chat history display
- âœ… Emotional state detection (joy, sad, anger, surprise, neutral)
- âœ… FastAPI backend + WebSocket communication
- âœ… GPU acceleration (CUDA) for TTS and LLM

### **Current System Architecture**
```
User Voice â†’ Browser Speech Recognition â†’ WebSocket â†’
â†’ LLM (Ollama qwen2.5:7b) â†’ Emotion Detection â†’
â†’ TTS (XTTS-v2) â†’ Audio Output
```

---

## ğŸ¯ **Today's Goal: Add 3D Visual Character**

### **What We're Building**
Integrate a 3D animated character that:
- Syncs lips to voice output (lip-sync)
- Displays emotions based on AI responses
- Shows idle animations (blinking, breathing)
- Runs smoothly on RTX 4060 GPU
- Uses FREE open-source tools only

---

## ğŸ› ï¸ **Open-Source Technology Stack**

### **Option A: VRoid Studio + VSeeFace (RECOMMENDED - Easiest)**

**Why This Option:**
- âœ… FREE and beginner-friendly
- âœ… No coding for character creation
- âœ… Excellent lip-sync quality
- âœ… Anime-style characters
- âœ… Works perfectly with Windows
- âœ… GPU-accelerated
- âœ… Can output to OBS for TV display

**Software Stack:**
```
VRoid Studio (Free)
â”œâ”€ Purpose: Create custom 3D anime character
â”œâ”€ Platform: Windows/Mac
â”œâ”€ Output: VRM file format
â”œâ”€ Time: 1-2 hours to create character
â””â”€ Link: https://vroid.com/en/studio

VSeeFace (Free)
â”œâ”€ Purpose: Animate VRM character with AI
â”œâ”€ Platform: Windows only
â”œâ”€ Features:
â”‚   â”œâ”€ Perfect lip-sync
â”‚   â”œâ”€ Expression control via API
â”‚   â”œâ”€ Idle animations built-in
â”‚   â”œâ”€ Low CPU/GPU usage
â”‚   â””â”€ VMC protocol support
â”œâ”€ Link: https://www.vseeface.icu/
â””â”€ Python Integration: VMC protocol (OSC/UDP)

OBS Studio (Free)
â”œâ”€ Purpose: Capture and display on TV
â”œâ”€ Platform: Windows/Mac/Linux
â””â”€ Link: https://obsproject.com/
```

**Architecture with VSeeFace:**
```
Python Backend
    â”œâ”€ Emotion Detection
    â”œâ”€ Audio Output (wzy.wav voice)
    â””â”€ VMC Protocol Messages
            â†“
    VSeeFace Application
    â”œâ”€ Receives VMC commands
    â”œâ”€ Lip-sync from audio
    â”œâ”€ Expression changes
    â””â”€ Idle animations
            â†“
    OBS Studio â†’ HDMI â†’ TV Display
```

---

### **Option B: Live2D Cubism + iFacialMocap (Anime 2D)**

**Why This Option:**
- âœ… Professional anime aesthetic
- âœ… More expressive than 3D
- âœ… Lower GPU requirements
- âš ï¸ Requires some character setup

**Software Stack:**
```
Live2D Cubism (Free Viewer, $8/month for Editor)
â”œâ”€ Purpose: Create 2D animated character
â”œâ”€ Platform: Windows/Mac
â”œâ”€ Output: .model3.json
â””â”€ Link: https://www.live2d.com/

VTube Studio (Free)
â”œâ”€ Purpose: Animate Live2D models
â”œâ”€ Platform: Windows/Mac/iOS/Android
â”œâ”€ Features:
â”‚   â”œâ”€ Expression hotkeys
â”‚   â”œâ”€ Python API available
â”‚   â””â”€ OBS integration
â””â”€ Link: https://denchisoft.com/
```

---

### **Option C: Blender + Rhubarb Lip-Sync (Full Control)**

**Why This Option:**
- âœ… Maximum customization
- âœ… Professional-quality output
- âš ï¸ Steeper learning curve
- âš ï¸ More development time

**Software Stack:**
```
Blender (Free)
â”œâ”€ Purpose: 3D character modeling & animation
â”œâ”€ Platform: Windows/Mac/Linux
â”œâ”€ Features: Everything
â””â”€ Link: https://www.blender.org/

Rhubarb Lip Sync (Free)
â”œâ”€ Purpose: Generate lip-sync from audio
â”œâ”€ Platform: Command-line tool
â”œâ”€ Output: Animation JSON
â””â”€ Link: https://github.com/DanielSWolf/rhubarb-lip-sync

Python Integration
â”œâ”€ bpy (Blender Python API)
â””â”€ Real-time scene control
```

---

## ğŸ“… **Tomorrow's Development Plan (Option A - VSeeFace)**

### **Phase 1: Setup (1-2 hours)**

**Task 1.1: Install Software**
```bash
Downloads needed:
1. VRoid Studio: https://vroid.com/en/studio
2. VSeeFace: https://www.vseeface.icu/
3. OBS Studio: https://obsproject.com/

Installation order:
1. VRoid Studio â†’ Create character
2. VSeeFace â†’ Test character
3. OBS Studio â†’ Capture for TV
```

**Task 1.2: Create Basic Character in VRoid**
```
Character creation steps:
1. Open VRoid Studio
2. Start from preset template
3. Customize:
   â”œâ”€ Hair style & color
   â”œâ”€ Eye shape & color
   â”œâ”€ Face shape
   â”œâ”€ Body proportions
   â”œâ”€ Outfit/clothing
   â””â”€ Accessories
4. Export as .vrm file
5. Save to: F:\Ani\character\ani_character.vrm

Time: 1-2 hours (can be refined later)
```

**Task 1.3: Test Character in VSeeFace**
```
VSeeFace setup:
1. Launch VSeeFace
2. Load ani_character.vrm
3. Test built-in features:
   â”œâ”€ Webcam tracking (for testing)
   â”œâ”€ Microphone lip-sync
   â”œâ”€ Expression hotkeys
   â””â”€ Idle animations
4. Configure VMC protocol:
   â”œâ”€ Enable VMC receiver
   â”œâ”€ Port: 39539 (default)
   â””â”€ Note the IP address
```

---

### **Phase 2: Python Integration (2-3 hours)**

**Task 2.1: Install VMC Protocol Library**
```bash
# Install python-osc for VMC communication
pip install python-osc
pip install asyncio
```

**Task 2.2: Create Animation Controller**
```python
# Claude will create: animation_controller.py

Features to implement:
â”œâ”€ VMC protocol client
â”œâ”€ Expression triggers:
â”‚   â”œâ”€ joy â†’ happy expression
â”‚   â”œâ”€ sad â†’ sad expression
â”‚   â”œâ”€ anger â†’ angry expression
â”‚   â”œâ”€ surprise â†’ surprised expression
â”‚   â””â”€ neutral â†’ neutral expression
â”œâ”€ Lip-sync trigger from audio playback
â””â”€ Blinking and idle animation coordination

VMC Protocol Messages:
- /VMC/Ext/Blend/Val (blend shape values)
- /VMC/Ext/Hmd/Pos (head position)
- /VMC/Ext/Blend/Apply (apply changes)
```

**Task 2.3: Integrate with Existing System**
```python
# Update main_full.py to add animation controller

Integration points:
1. When LLM generates emotion:
   â””â”€ Send expression command to VSeeFace

2. When TTS generates audio:
   â”œâ”€ Save audio file temporarily
   â”œâ”€ Send audio path to VSeeFace for lip-sync
   â””â”€ Play audio synchronized

3. Idle state:
   â””â”€ VSeeFace handles automatically (blinking, breathing)
```

---

### **Phase 3: OBS Integration (1 hour)**

**Task 3.1: Configure OBS Scene**
```
OBS Scene Setup:
1. Create new scene: "Ani Display"
2. Add sources:
   â”œâ”€ Window Capture â†’ VSeeFace window
   â”œâ”€ Background image/video (optional)
   â”œâ”€ Audio output capture â†’ System audio
   â””â”€ Filters:
       â”œâ”€ Chroma Key (if using green screen)
       â”œâ”€ Color correction
       â””â”€ Scaling/Transform
3. Set canvas resolution:
   â”œâ”€ 1920x1080 (Full HD) or
   â””â”€ 3840x2160 (4K for TV)
4. Output: Fullscreen to TV via HDMI
```

**Task 3.2: Test Full Pipeline**
```
End-to-end test:
1. Start VSeeFace with character loaded
2. Start Python backend (main_full.py)
3. Start OBS with capture scene
4. Open browser to http://localhost:8000
5. Test conversation:
   User: "ä½ å¥½" (Chinese)
   â†“
   AI thinks (purple status)
   â†“
   AI responds with emotion (joy)
   â†“
   Character shows happy expression
   â†“
   Character lip-syncs to voice
   â†“
   Returns to idle state
```

---

### **Phase 4: Polish & Optimization (1-2 hours)**

**Task 4.1: Expression Mapping**
```python
# Refine emotion â†’ expression mapping

Current emotions from LLM:
â”œâ”€ joy (0.0-1.0 intensity)
â”œâ”€ sad (0.0-1.0 intensity)
â”œâ”€ anger (0.0-1.0 intensity)
â”œâ”€ surprise (0.0-1.0 intensity)
â””â”€ neutral (0.0-1.0 intensity)

VSeeFace expressions:
â”œâ”€ Happy (blend value 0-1)
â”œâ”€ Sad (blend value 0-1)
â”œâ”€ Angry (blend value 0-1)
â”œâ”€ Surprised (blend value 0-1)
â”œâ”€ Neutral (blend value 0-1)
â”œâ”€ Blinking (automatic)
â””â”€ Mouth (lip-sync automatic)

Intensity mapping:
- Low intensity (0.0-0.5) â†’ 50% expression blend
- High intensity (0.5-1.0) â†’ 100% expression blend
```

**Task 4.2: Performance Tuning**
```python
Optimizations:
â”œâ”€ Async expression updates (don't block TTS)
â”œâ”€ Expression queue (smooth transitions)
â”œâ”€ Debounce rapid expression changes
â””â”€ Cache common expressions

Target performance:
â”œâ”€ Expression change: <100ms
â”œâ”€ Lip-sync delay: <50ms
â”œâ”€ Total response: Still ~5-9 seconds
â””â”€ FPS in VSeeFace: 60fps
```

**Task 4.3: Error Handling**
```python
Edge cases to handle:
â”œâ”€ VSeeFace not running â†’ Graceful degradation
â”œâ”€ VMC connection lost â†’ Auto-reconnect
â”œâ”€ Expression conflicts â†’ Priority system
â”œâ”€ Audio sync issues â†’ Adjust timing
â””â”€ OBS crash â†’ Restart integration
```

---

## ğŸ“ **Detailed Task List for Claude Tomorrow**

### **Morning Session (2-3 hours)**

1. **Install and test VRoid Studio + VSeeFace**
   - Download software
   - Create basic character in VRoid
   - Export .vrm file
   - Load and test in VSeeFace
   - Configure VMC protocol

2. **Create animation_controller.py**
   ```python
   # Claude should create this file with:

   class AnimationController:
       def __init__(self, vmc_host="127.0.0.1", vmc_port=39539):
           """Initialize VMC OSC client"""

       async def set_expression(self, emotion: str, intensity: float):
           """Send expression command to VSeeFace"""

       async def trigger_lipsync(self, audio_file: str):
           """Trigger lip-sync for audio file"""

       async def reset_to_idle(self):
           """Return to neutral/idle state"""
   ```

3. **Integrate with main_full.py**
   - Add animation controller initialization
   - Hook into LLM emotion detection
   - Sync with TTS audio output
   - Test with simple conversation

---

### **Afternoon Session (2-3 hours)**

4. **Set up OBS Studio**
   - Install OBS
   - Create capture scene
   - Configure window capture for VSeeFace
   - Add background and effects
   - Set output to TV via HDMI

5. **Test full pipeline**
   - Run end-to-end conversation test
   - Verify expression changes
   - Check lip-sync quality
   - Confirm TV display working
   - Debug any issues

6. **Polish and optimize**
   - Refine expression intensity mapping
   - Smooth expression transitions
   - Optimize performance
   - Add error handling
   - Document the system

---

## ğŸ¯ **Success Criteria for Tomorrow**

By end of day, you should have:

âœ… **Visual Character**
- Custom anime character created in VRoid
- Character displays in VSeeFace
- Character visible on TV via OBS

âœ… **Emotion Expressions**
- AI emotions trigger character expressions
- Smooth transitions between expressions
- Intensity levels working correctly

âœ… **Lip-Sync**
- Character mouth syncs to voice output
- Minimal delay (<100ms)
- Works for both Chinese and English

âœ… **Full System Integration**
- Voice input â†’ AI response â†’ Expression + Lip-sync
- All components working together
- No crashes or major bugs

âœ… **Documentation**
- Code documented and clean
- Setup guide for VSeeFace integration
- Troubleshooting notes

---

## ğŸ“¦ **Required Downloads**

Save these links for tomorrow:

1. **VRoid Studio**: https://vroid.com/en/studio
   - Size: ~200MB
   - Platform: Windows 10/11

2. **VSeeFace**: https://www.vseeface.icu/
   - Size: ~150MB
   - Platform: Windows only
   - Requires: .NET Framework

3. **OBS Studio**: https://obsproject.com/
   - Size: ~100MB
   - Platform: Windows/Mac/Linux

4. **Python OSC Library**:
   ```bash
   pip install python-osc
   ```

---

## ğŸš€ **Tomorrow's Prompt for Claude**

Copy and paste this to Claude Code tomorrow morning:

```
Hi Claude! Today I'm adding 3D visual character animation to my Ani AI Voice Companion.

**Current Project Status:**
- Repository: https://github.com/LookJohnny/AIWaifu
- Location: F:\Ani\
- Server: http://localhost:8000
- Working: Voice conversation with emotion detection

**Today's Goal:**
Integrate VSeeFace + VRoid Studio for 3D character animation with:
1. Lip-sync to TTS voice output (wzy.wav voice)
2. Expression changes based on AI emotions
3. Display on TV via OBS Studio
4. Full integration with existing FastAPI backend

**What I Need Your Help With:**

**Phase 1: Setup & Character Creation (Morning)**
1. Guide me through VRoid Studio character creation
2. Help me export and test in VSeeFace
3. Configure VMC protocol for Python control

**Phase 2: Python Integration (Afternoon)**
1. Create animation_controller.py for VMC communication
2. Integrate with main_full.py emotion detection
3. Sync lip-sync with TTS audio output
4. Handle expression transitions smoothly

**Phase 3: OBS & Testing (Evening)**
1. Set up OBS scene for TV display
2. Test full conversation pipeline
3. Debug and optimize performance
4. Document everything

**Tech Stack:**
- VRoid Studio (character creation)
- VSeeFace (animation + lip-sync)
- python-osc (VMC protocol)
- OBS Studio (TV display)
- Existing: FastAPI + qwen2.5:7b + XTTS-v2

**Current Emotion System:**
The LLM already outputs: joy, sad, anger, surprise, neutral (with intensity 0.0-1.0)
These need to map to VSeeFace blend shapes.

**System Specs:**
- Windows 11
- RTX 4060 8GB
- Python 3.11

Please guide me step-by-step. Write all necessary code, explain how it works, and help me debug any issues. Let's build this! ğŸš€
```

---

## ğŸ’¡ **Alternative: Quick Prototype Path**

If VSeeFace setup is too complex, here's a simpler fallback:

### **Fallback Option: Ready Player Me + Three.js**

```javascript
// Web-based 3D character (runs in browser)

Stack:
â”œâ”€ Ready Player Me (free 3D avatars)
â”œâ”€ Three.js (3D rendering in browser)
â”œâ”€ rhubarb-lip-sync (audio â†’ visemes)
â””â”€ Existing FastAPI backend

Advantages:
âœ“ No extra software installation
âœ“ Works in same browser as UI
âœ“ Cross-platform (works anywhere)
âœ“ Easier to deploy

Disadvantages:
âœ— More coding required
âœ— Less anime aesthetic
âœ— May need performance optimization
```

Claude can implement this if VSeeFace path has issues.

---

## âœ… **Final Checklist for Tomorrow**

Before starting, make sure:
- [ ] Current server (b7e1c8) is still running properly
- [ ] GitHub is up to date
- [ ] You have ~4-6 hours available
- [ ] TV/monitor is ready for testing
- [ ] Headphones/speakers working
- [ ] Microphone working

During development:
- [ ] Take notes on what works/doesn't work
- [ ] Ask Claude to explain anything unclear
- [ ] Test frequently (don't wait until end)
- [ ] Save/commit progress regularly

---

## ğŸ‰ **Expected Outcome**

By tomorrow evening, you'll have:

**Ani - Full Multimodal AI Companion**
- ğŸ—£ï¸ Voice conversation (already working)
- ğŸ­ 3D animated character
- ğŸ˜Š Emotional expressions
- ğŸ‘„ Lip-sync to voice
- ğŸ“º Beautiful display on TV
- ğŸ¨ Custom anime character design
- ğŸ’¾ All code on GitHub

**This will be AMAZING!** ğŸš€

Good luck tomorrow! Let's bring Ani to life visually! ğŸ’œ
```

---

## ğŸ“Š **Time Estimate**

| Phase | Time | Difficulty |
|-------|------|------------|
| Software setup + character creation | 2-3 hours | Easy |
| Python VMC integration | 2-3 hours | Medium |
| OBS setup + testing | 1-2 hours | Easy |
| Polish + debug | 1-2 hours | Medium |
| **Total** | **6-10 hours** | **Medium** |

Perfect for a full day of development! ğŸ¯
