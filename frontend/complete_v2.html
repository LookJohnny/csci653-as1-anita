<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ANITA - AI Voice Companion</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&family=Orbitron:wght@700;900&display=swap');

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: #000;
            overflow: hidden;
            color: white;
        }

        /* ===== BACKGROUND LAYERS ===== */
        .bg-layer {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        #bg-gradient {
            z-index: 1;
            background: linear-gradient(-45deg, #1a1a2e, #667eea, #764ba2, #f093fb);
            background-size: 400% 400%;
            animation: gradientFlow 15s ease infinite;
        }

        @keyframes gradientFlow {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        /* 3D Canvas Container */
        #canvas-container {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 2;
        }

        /* ===== UI OVERLAY LAYER ===== */
        .ui-layer {
            position: fixed;
            z-index: 10;
            pointer-events: none;
        }

        .ui-layer > * {
            pointer-events: auto;
        }

        /* Header - Logo & Status */
        #header {
            top: 0;
            left: 0;
            width: 100%;
            padding: 24px 32px;
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
        }

        #logo {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }

        .logo-main {
            font-family: 'Orbitron', monospace;
            font-size: 36px;
            font-weight: 900;
            letter-spacing: 4px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            filter: drop-shadow(0 0 20px rgba(102, 126, 234, 0.4));
        }

        .logo-sub {
            font-size: 9px;
            font-weight: 600;
            letter-spacing: 4px;
            color: rgba(255, 255, 255, 0.6);
            text-transform: uppercase;
        }

        /* Status Card */
        #status-card {
            background: rgba(15, 15, 25, 0.7);
            backdrop-filter: blur(20px);
            border: 1px solid rgba(102, 126, 234, 0.2);
            border-radius: 16px;
            padding: 16px 20px;
            display: flex;
            align-items: center;
            gap: 12px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.4);
        }

        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #4ade80;
            box-shadow: 0 0 12px #4ade80;
            animation: pulse-dot 2s ease-in-out infinite;
        }

        @keyframes pulse-dot {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        .status-dot.thinking {
            background: #fbbf24;
            box-shadow: 0 0 12px #fbbf24;
        }

        .status-dot.disconnected {
            background: #ef4444;
            box-shadow: 0 0 12px #ef4444;
        }

        #status-text {
            font-size: 13px;
            font-weight: 600;
            color: rgba(255, 255, 255, 0.9);
        }

        /* Emotion Display */
        #emotion-panel {
            top: 24px;
            right: 32px;
        }

        #emotion-display {
            background: rgba(15, 15, 25, 0.7);
            backdrop-filter: blur(20px);
            border: 1px solid rgba(102, 126, 234, 0.2);
            border-radius: 16px;
            padding: 20px 24px;
            min-width: 180px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.4);
        }

        .emotion-label {
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 1.5px;
            color: rgba(255, 255, 255, 0.5);
            margin-bottom: 12px;
        }

        .emotion-content {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-bottom: 12px;
        }

        .emotion-icon {
            font-size: 28px;
            filter: drop-shadow(0 2px 8px rgba(0, 0, 0, 0.3));
        }

        .emotion-text {
            font-size: 16px;
            font-weight: 600;
            color: white;
        }

        .emotion-bar {
            width: 100%;
            height: 4px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 2px;
            overflow: hidden;
        }

        .emotion-bar-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea, #764ba2);
            width: 0%;
            transition: width 0.5s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: 0 0 8px rgba(102, 126, 234, 0.6);
        }

        /* Bottom Controls */
        #bottom-controls {
            bottom: 32px;
            left: 50%;
            transform: translateX(-50%);
        }

        #controls {
            background: rgba(15, 15, 25, 0.85);
            backdrop-filter: blur(30px);
            border: 1px solid rgba(102, 126, 234, 0.3);
            border-radius: 60px;
            padding: 20px 40px;
            display: flex;
            align-items: center;
            gap: 20px;
            box-shadow: 0 12px 48px rgba(0, 0, 0, 0.5);
        }

        #talk-btn {
            padding: 16px 40px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border: none;
            border-radius: 40px;
            color: white;
            font-size: 15px;
            font-weight: 600;
            font-family: 'Inter', sans-serif;
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: 0 4px 20px rgba(102, 126, 234, 0.4);
            position: relative;
            overflow: hidden;
        }

        #talk-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(102, 126, 234, 0.6);
        }

        #talk-btn:active {
            transform: translateY(0);
        }

        #talk-btn.recording {
            background: linear-gradient(135deg, #ef4444, #dc2626);
            box-shadow: 0 4px 20px rgba(239, 68, 68, 0.5);
            animation: pulse-btn 1.5s ease-in-out infinite;
        }

        @keyframes pulse-btn {
            0%, 100% {
                transform: scale(1);
                box-shadow: 0 4px 20px rgba(239, 68, 68, 0.5);
            }
            50% {
                transform: scale(1.02);
                box-shadow: 0 8px 35px rgba(239, 68, 68, 0.7);
            }
        }

        #mic-status {
            font-size: 13px;
            font-weight: 500;
            color: rgba(255, 255, 255, 0.7);
        }

        /* Loading Screen */
        #loading {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 100;
            background: #000;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            gap: 24px;
        }

        .spinner {
            width: 48px;
            height: 48px;
            border: 3px solid rgba(102, 126, 234, 0.2);
            border-top-color: #667eea;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }

        .loading-text {
            font-size: 14px;
            font-weight: 600;
            color: rgba(255, 255, 255, 0.6);
            letter-spacing: 2px;
        }

        /* Debug Panel */
        #debug-panel {
            bottom: 120px;
            left: 32px;
            background: rgba(0, 0, 0, 0.8);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(102, 126, 234, 0.2);
            border-radius: 12px;
            padding: 16px;
            font-family: 'Courier New', monospace;
            font-size: 11px;
            color: #4ade80;
            min-width: 280px;
        }

        .debug-title {
            color: #667eea;
            font-weight: bold;
            margin-bottom: 8px;
            font-size: 12px;
        }

        .debug-line {
            margin: 4px 0;
            opacity: 0.9;
        }
    </style>
</head>
<body>
    <!-- Background -->
    <div class="bg-layer" id="bg-gradient"></div>

    <!-- 3D Character Canvas -->
    <div id="canvas-container"></div>

    <!-- UI Overlay -->
    <div class="ui-layer" id="header">
        <div id="logo">
            <div class="logo-main">ANITA</div>
            <div class="logo-sub">BY JOHNNY LIU</div>
        </div>
        <div id="status-card">
            <div class="status-dot"></div>
            <span id="status-text">Ready</span>
        </div>
    </div>

    <div class="ui-layer" id="emotion-panel" style="display: none;">
        <div id="emotion-display">
            <div class="emotion-label">Current Emotion</div>
            <div class="emotion-content">
                <span class="emotion-icon" id="emotion-icon">üòä</span>
                <span class="emotion-text" id="emotion-text">Neutral</span>
            </div>
            <div class="emotion-bar">
                <div class="emotion-bar-fill" id="emotion-bar-fill"></div>
            </div>
        </div>
    </div>

    <div class="ui-layer" id="bottom-controls" style="display: none;">
        <div id="controls">
            <button id="talk-btn">üé§ Start Listening</button>
            <span id="mic-status">Click to start/stop listening</span>
        </div>
    </div>

    <div class="ui-layer" id="debug-panel" style="display: none;">
        <div class="debug-title">üîß DEBUG INFO</div>
        <div class="debug-line">Gesture: <span id="debug-gesture">none</span></div>
        <div class="debug-line">Progress: <span id="debug-progress">0.00</span></div>
        <div class="debug-line">R-Arm: <span id="debug-rarm">0, 0, 0</span></div>
        <div class="debug-line">L-Arm: <span id="debug-larm">0, 0, 0</span></div>
        <div class="debug-line">Head: <span id="debug-head">0, 0, 0</span></div>
    </div>

    <!-- Loading Screen -->
    <div id="loading">
        <div class="spinner"></div>
        <div class="loading-text">LOADING ANITA...</div>
    </div>

    <script type="importmap">
    {
        "imports": {
            "three": "https://cdn.jsdelivr.net/npm/three@0.167.0/build/three.module.js",
            "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.167.0/examples/jsm/",
            "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3.1.2/lib/three-vrm.module.js"
        }
    }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { VRMLoaderPlugin } from '@pixiv/three-vrm';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';

        // Global state
        let scene, camera, renderer, controls, vrm;
        let ws = null;
        let recognition = null;
        let isListening = false;
        let currentAudio = null;
        const clock = new THREE.Clock();
        let time = 0;

        // Mic streaming state (WS audio)
        let audioContext = null;
        let micStream = null;
        let sourceNode = null;
        let processorNode = null;
        let downsampleBuffer = [];
        const IN_SAMPLE_RATE = 48000;
        const OUT_SAMPLE_RATE = 16000;
        const CHUNK_SAMPLES = 512; // 32ms @16kHz

        // Expression transition state
        let currentExpression = { emotion: 'neutral', intensity: 0 };
        let targetExpression = { emotion: 'neutral', intensity: 0 };
        let expressionTransitionProgress = 1.0;
        const EXPRESSION_TRANSITION_SPEED = 3.0;

        // Blink system
        let lastBlinkTime = 0;
        let nextBlinkDelay = 3.0;
        let isBlinking = false;
        let blinkProgress = 0;

        // Gesture system
        let currentGesture = null;
        let gestureProgress = 0;

        const EMOTION_MAP = {
            'joy': 'happy',
            'sad': 'sad',
            'anger': 'angry',
            'surprise': 'surprised',
            'neutral': 'neutral',
            'excited': 'happy',
            'confused': 'surprised',
            'embarrassed': 'happy',
            'determined': 'neutral',
            'relaxed': 'relaxed'
        };

        // ÂàùÂßãÂåñÂú∫ÊôØ
        scene = new THREE.Scene();
        scene.background = null; // Transparent to show gradient background

        camera = new THREE.PerspectiveCamera(30, window.innerWidth / window.innerHeight, 0.1, 100);
        camera.position.set(0, 1.4, 3);

        renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.outputColorSpace = THREE.SRGBColorSpace;
        document.getElementById('canvas-container').appendChild(renderer.domElement);

        // ÁÅØÂÖâ
        const light1 = new THREE.DirectionalLight(0xffffff, 1.5);
        light1.position.set(1, 1, 1);
        scene.add(light1);

        const light2 = new THREE.AmbientLight(0xffffff, 0.8);
        scene.add(light2);

        const rimLight = new THREE.DirectionalLight(0x667eea, 0.5);
        rimLight.position.set(-1, 1, -1);
        scene.add(rimLight);

        // ÊéßÂà∂Âô®
        controls = new OrbitControls(camera, renderer.domElement);
        controls.target.set(0, 1.2, 0);
        controls.enableDamping = true;
        controls.dampingFactor = 0.05;
        controls.update();

        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        // Âä†ËΩΩVRM
        const loader = new GLTFLoader();
        loader.register((parser) => new VRMLoaderPlugin(parser));

        loader.load('/character/darkhair.vrm', (gltf) => {
            vrm = gltf.userData.vrm;
            scene.add(vrm.scene);
            vrm.scene.rotation.y = Math.PI;

            // Fix T-pose arms - set natural resting position
            try {
                if (vrm && vrm.humanoid) {
                    const lUpper = vrm.humanoid.getNormalizedBoneNode('leftUpperArm');
                    const rUpper = vrm.humanoid.getNormalizedBoneNode('rightUpperArm');
                    const lLower = vrm.humanoid.getNormalizedBoneNode('leftLowerArm');
                    const rLower = vrm.humanoid.getNormalizedBoneNode('rightLowerArm');

                    // Classic anime girl standing pose - arms naturally at sides
                    if (lUpper) lUpper.rotation.set(-0.1, 0, 1.5);   // X: slightly back, Z: down at side
                    if (rUpper) rUpper.rotation.set(-0.1, 0, -1.5);  // Mirror for right arm
                    if (lLower) lLower.rotation.set(0, 0, 0.2);      // Natural elbow bend
                    if (rLower) rLower.rotation.set(0, 0, -0.2);

                    // CRITICAL: Capture the corrected base rotations for idle animation
                    captureBaseRot();
                }
            } catch (e) {
                console.warn('Failed to fix T-pose:', e);
            }

            document.getElementById('loading').style.display = 'none';
            document.getElementById('bottom-controls').style.display = 'block';
            document.getElementById('emotion-panel').style.display = 'block';
            document.getElementById('debug-panel').style.display = 'block';
        });

        // Ë°®ÊÉÖÊéßÂà∂
        function setExpression(emotion, intensity = 1.0) {
            if (!vrm || !vrm.expressionManager) return;

            targetExpression = { emotion, intensity };
            expressionTransitionProgress = 0;

            const emotionIcons = {
                'joy': 'üòä', 'excited': 'ü§©', 'sad': 'üò¢', 'anger': 'üò†',
                'surprise': 'üò≤', 'confused': 'ü§î', 'embarrassed': 'üò≥',
                'determined': 'üò§', 'relaxed': 'üòå', 'neutral': 'üòê'
            };

            document.getElementById('emotion-text').textContent =
                emotion.charAt(0).toUpperCase() + emotion.slice(1);
            document.getElementById('emotion-icon').textContent = emotionIcons[emotion] || 'üòä';
            document.getElementById('emotion-bar-fill').style.width = (intensity * 100) + '%';
        }

        function updateExpression(deltaTime) {
            if (!vrm || !vrm.expressionManager) return;

            if (expressionTransitionProgress < 1.0) {
                expressionTransitionProgress = Math.min(1.0, expressionTransitionProgress + deltaTime * EXPRESSION_TRANSITION_SPEED);
            }

            const t = easeInOutCubic(expressionTransitionProgress);
            const blendedIntensity = currentExpression.intensity * (1 - t) + targetExpression.intensity * t;
            const expressionName = EMOTION_MAP[targetExpression.emotion] || 'neutral';

            try {
                ['happy', 'sad', 'angry', 'surprised', 'neutral', 'relaxed'].forEach(exp => {
                    if (vrm.expressionManager.getValue(exp) !== undefined) {
                        vrm.expressionManager.setValue(exp, 0);
                    }
                });
                vrm.expressionManager.setValue(expressionName, blendedIntensity);

                if (expressionTransitionProgress >= 1.0) {
                    currentExpression = { ...targetExpression };
                }
            } catch (error) {
                console.error('[ERROR] Expression update:', error);
            }
        }

        function easeInOutCubic(t) {
            return t < 0.5 ? 4 * t * t * t : 1 - Math.pow(-2 * t + 2, 3) / 2;
        }

        function updateBlink(deltaTime) {
            if (!vrm || !vrm.expressionManager) return;

            if (isBlinking) {
                blinkProgress += deltaTime * 10;
                if (blinkProgress >= 1.0) {
                    isBlinking = false;
                    blinkProgress = 0;
                    lastBlinkTime = time;
                    nextBlinkDelay = 2.0 + Math.random() * 3.0;
                }

                const blinkValue = Math.sin(blinkProgress * Math.PI);
                try {
                    vrm.expressionManager.setValue('blink', blinkValue);
                } catch (e) {}
            } else {
                if (time - lastBlinkTime > nextBlinkDelay) {
                    isBlinking = true;
                    blinkProgress = 0;
                }
            }
        }

        function setViseme(viseme, weight = 1.0) {
            if (!vrm || !vrm.expressionManager) return;

            const visemeMap = { 'A': 'aa', 'E': 'ee', 'I': 'ih', 'O': 'oh', 'U': 'ou' };
            const vrmViseme = visemeMap[viseme] || 'aa';

            try {
                Object.values(visemeMap).forEach(v => {
                    vrm.expressionManager.setValue(v, 0);
                });
                vrm.expressionManager.setValue(vrmViseme, weight);
            } catch (error) {}
        }

        function playGesture(gesture) {
            if (!vrm || !vrm.humanoid) return;
            currentGesture = gesture;
            gestureProgress = 0;
        }

        function updateGesture(deltaTime) {
            if (!currentGesture || !vrm || !vrm.humanoid) {
                document.getElementById('debug-gesture').textContent = 'none';
                return;
            }

            gestureProgress += deltaTime * 1.5;

            const rightArm = vrm.humanoid.getNormalizedBoneNode('rightUpperArm');
            const head = vrm.humanoid.getNormalizedBoneNode('head');

            document.getElementById('debug-gesture').textContent = currentGesture;
            document.getElementById('debug-progress').textContent = gestureProgress.toFixed(2);

            const t = Math.min(1.0, gestureProgress / 1.5);
            const easeIn = easeInOutCubic(t);
            const easeOut = easeInOutCubic(Math.max(0, (gestureProgress - 1.5) / 0.5));

            try {
                switch(currentGesture) {
                    case 'wave':
                        if (rightArm) {
                            const wave = Math.sin(gestureProgress * 8) * 0.3;
                            rightArm.rotation.set(0, 0, -1.3 + wave * (1 - easeOut));
                        }
                        break;
                    case 'nod':
                        if (head) {
                            const nods = Math.sin(gestureProgress * 5) * 0.2;
                            head.rotation.x = nods * (1 - easeOut);
                            head.rotation.y = 0;
                            head.rotation.z = 0;
                        }
                        break;
                    case 'shake_head':
                        if (head) {
                            const shakes = Math.sin(gestureProgress * 6) * 0.25;
                            head.rotation.x = 0;
                            head.rotation.y = shakes * (1 - easeOut);
                            head.rotation.z = 0;
                        }
                        break;
                    case 'think':
                        if (head) {
                            head.rotation.x = -0.15 * easeIn;
                            head.rotation.y = 0.1 * easeIn;
                            head.rotation.z = 0;
                        }
                        break;
                    case 'celebrate':
                        if (head) {
                            const bounce = Math.sin(gestureProgress * 6) * 0.1;
                            head.rotation.x = bounce;
                            head.rotation.y = 0;
                            head.rotation.z = 0;
                        }
                        break;
                }

                if (rightArm) {
                    document.getElementById('debug-rarm').textContent =
                        `${rightArm.rotation.x.toFixed(2)}, ${rightArm.rotation.y.toFixed(2)}, ${rightArm.rotation.z.toFixed(2)}`;
                }
                if (head) {
                    document.getElementById('debug-head').textContent =
                        `${head.rotation.x.toFixed(2)}, ${head.rotation.y.toFixed(2)}, ${head.rotation.z.toFixed(2)}`;
                }

                if (gestureProgress > 2.0) {
                    currentGesture = null;
                    gestureProgress = 0;
                }
            } catch (error) {
                console.error('[ERROR] Gesture:', error);
                currentGesture = null;
            }
        }

        // WebSocket
        function connectWebSocket() {
            if (ws && (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING)) return;

            ws = new WebSocket('ws://localhost:8000/ws');

            ws.onopen = () => {
                updateStatus('connected', 'Ready');
                // User manually clicks button to start listening
            };

            ws.onmessage = (event) => {
                const data = JSON.parse(event.data);
                console.log('[WS] Received message type:', data.type);

                if (data.type === 'emotion') {
                    console.log('[WS] Emotion:', data.emotion, 'Intensity:', data.intensity);
                    setExpression(data.emotion, data.intensity || 1.0);
                }
                if (data.type === 'audio') {
                    console.log('[WS] Audio received, length:', data.audio ? data.audio.length : 0);
                    playAudio(data.audio);
                }
                if (data.type === 'gesture' && data.gesture) {
                    console.log('[WS] Gesture:', data.gesture);
                    playGesture(data.gesture);
                }
                if (data.type === 'state') {
                    console.log('[WS] State:', data.value);
                    updateStatus('connected', data.value);
                    aniState = data.value;
                }
            };

            ws.onerror = (error) => {
                updateStatus('disconnected', 'Error');
            };

            ws.onclose = () => {
                updateStatus('disconnected', 'Disconnected');
                ws = null;
                setTimeout(connectWebSocket, 3000);
            };
        }

        function updateStatus(status, text) {
            const statusDot = document.querySelector('.status-dot');
            statusDot.className = 'status-dot';

            if (status === 'connected') statusDot.classList.add('connected');
            else if (status === 'thinking') statusDot.classList.add('thinking');
            else statusDot.classList.add('disconnected');

            document.getElementById('status-text').textContent = text;
        }

        async function playAudio(audioBase64) {
            console.log('[AUDIO] Received audio data, length:', audioBase64.length);

            if (currentAudio) {
                console.log('[AUDIO] Stopping previous audio');
                currentAudio.pause();
                currentAudio = null;
            }
            stopMouth();

            try {
                const audioData = atob(audioBase64);
                const arrayBuffer = new ArrayBuffer(audioData.length);
                const view = new Uint8Array(arrayBuffer);
                for (let i = 0; i < audioData.length; i++) {
                    view[i] = audioData.charCodeAt(i);
                }

                const blob = new Blob([arrayBuffer], { type: 'audio/wav' });
                const audioUrl = URL.createObjectURL(blob);
                console.log('[AUDIO] Created blob URL:', audioUrl);

                currentAudio = new Audio(audioUrl);
                currentAudio.volume = 1.0; // Á°Æ‰øùÈü≥ÈáèÊòØÊúÄÂ§ß

                currentAudio.addEventListener('loadeddata', () => {
                    console.log('[AUDIO] Audio loaded, duration:', currentAudio.duration);
                });

                currentAudio.addEventListener('playing', () => {
                    console.log('[OK] Audio playing');
                    animateMouth();
                });

                currentAudio.addEventListener('ended', () => {
                    console.log('[AUDIO] Audio ended');
                    stopMouth();
                    updateStatus('connected', 'Ready');
                    URL.revokeObjectURL(audioUrl);
                });

                currentAudio.addEventListener('error', (e) => {
                    console.error('[ERROR] Audio playback error:', e);
                    console.error('[ERROR] Audio error code:', currentAudio.error ? currentAudio.error.code : 'unknown');
                    stopMouth();
                    updateStatus('connected', 'Ready');
                    URL.revokeObjectURL(audioUrl);
                });

                console.log('[AUDIO] Attempting to play...');
                await currentAudio.play();
                console.log('[OK] Play initiated successfully');
            } catch (error) {
                console.error('[ERROR] Audio playback failed:', error);
                console.error('[ERROR] Error name:', error.name);
                console.error('[ERROR] Error message:', error.message);

                // Â¶ÇÊûúÊòØËá™Âä®Êí≠ÊîæË¢´ÈòªÊ≠¢
                if (error.name === 'NotAllowedError') {
                    alert('Èü≥È¢ëÊí≠ÊîæË¢´ÊµèËßàÂô®ÈòªÊ≠¢\n\nËß£ÂÜ≥ÊñπÊ≥ï:\n1. ÁÇπÂáªÈ°µÈù¢‰ªªÊÑè‰ΩçÁΩÆÊøÄÊ¥ªÈü≥È¢ë\n2. ÊàñÂú®ÊµèËßàÂô®ËÆæÁΩÆ‰∏≠ÂÖÅËÆ∏Ëá™Âä®Êí≠Êîæ\n\nËØ∑Âà∑Êñ∞È°µÈù¢Âπ∂ÂÖàÁÇπÂáª‰∏Ä‰∏ã‰ªªÊÑèÂú∞ÊñπÂÜç‰ΩøÁî®ËØ≠Èü≥ÂäüËÉΩ');
                }

                stopMouth();
                updateStatus('connected', 'Ready');
            }
        }

        let mouthAnimationFrame = null;
        function animateMouth() {
            const visemes = ['A', 'I', 'U', 'E', 'O'];
            let index = 0;
            function animate() {
                setViseme(visemes[index], 0.7);
                index = (index + 1) % visemes.length;
                mouthAnimationFrame = setTimeout(animate, 100);
            }
            animate();
        }

        function stopMouth() {
            if (mouthAnimationFrame) {
                clearTimeout(mouthAnimationFrame);
                mouthAnimationFrame = null;
            }
            setViseme('A', 0);
        }

        // ËØ≠Èü≥ËØÜÂà´ÔºàWeb Speech APIÔºâ
        function initSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SpeechRecognition) {
                console.log('[INFO] Speech Recognition not supported');
                return;
            }

            recognition = new SpeechRecognition();
            recognition.continuous = true;  // ÊåÅÁª≠ÁõëÂê¨Ê®°Âºè
            recognition.interimResults = false;
            recognition.lang = 'en-US';  // Ëã±ÊñáÊ®°Âºè

            recognition.onstart = () => {
                console.log('[OK] Speech recognition started');
                isListening = true;
                document.getElementById('talk-btn').classList.add('recording');
                document.getElementById('mic-status').textContent = 'üéß Listening continuously...';
                updateStatus('thinking', 'Listening...');
            };

            recognition.onresult = (event) => {
                const transcript = event.results[event.results.length - 1][0].transcript;
                console.log('[SPEECH] Recognized:', transcript);
                sendTextToAI(transcript);
            };

            recognition.onerror = (event) => {
                console.error('[ERROR] Speech recognition error:', event.error);
                if (event.error === 'no-speech') {
                    console.log('[INFO] No speech detected, continuing...');
                    return; // ÁªßÁª≠ÁõëÂê¨
                }
                updateStatus('connected', 'Ready');
                isListening = false;
                document.getElementById('talk-btn').classList.remove('recording');
            };

            recognition.onend = () => {
                console.log('[INFO] Speech recognition ended');
                // Â¶ÇÊûúËøòÂú®ÁõëÂê¨Áä∂ÊÄÅÔºåËá™Âä®ÈáçÂêØÔºàÂÆûÁé∞ÊåÅÁª≠ÁõëÂê¨Ôºâ
                if (isListening) {
                    console.log('[INFO] Restarting speech recognition...');
                    setTimeout(() => {
                        if (isListening) recognition.start();
                    }, 100);
                } else {
                    document.getElementById('talk-btn').classList.remove('recording');
                    document.getElementById('mic-status').textContent = 'Click to start/stop listening';
                }
            };
        }

        function startSpeechRecognition() {
            if (!recognition) {
                initSpeechRecognition();
            }
            if (recognition) {
                try {
                    recognition.start();
                } catch (e) {
                    console.error('[ERROR] Failed to start recognition:', e);
                }
            }
        }

        function stopSpeechRecognition() {
            if (recognition) {
                isListening = false;
                try {
                    recognition.stop();
                } catch (e) {
                    console.error('[ERROR] Failed to stop recognition:', e);
                }
            }
        }

        // ===== Streaming microphone over WebSocket =====
        function startMicStreaming() {
            console.log('[DEBUG] startMicStreaming called');

            if (!ws || ws.readyState !== WebSocket.OPEN) {
                console.error('[ERROR] WebSocket not connected:', ws ? ws.readyState : 'null');
                updateStatus('disconnected', 'Not connected');
                return;
            }

            if (processorNode) {
                console.log('[DEBUG] Already streaming');
                return; // already running
            }

            const constraints = {
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    channelCount: 1,
                    sampleRate: IN_SAMPLE_RATE
                }
            };

            console.log('[DEBUG] Requesting microphone access...');
            navigator.mediaDevices.getUserMedia(constraints).then(stream => {
                console.log('[OK] Microphone access granted');
                micStream = stream;
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: IN_SAMPLE_RATE });
                sourceNode = audioContext.createMediaStreamSource(stream);
                const bufferSize = 2048;
                processorNode = audioContext.createScriptProcessor(bufferSize, 1, 1);

                processorNode.onaudioprocess = (e) => {
                    const input = e.inputBuffer.getChannelData(0);
                    const down = downsample(input, IN_SAMPLE_RATE, OUT_SAMPLE_RATE);
                    if (down && down.length) {
                        downsampleBuffer = downsampleBuffer.concat(Array.from(down));
                        // Send fixed-size chunks of 512 samples @16kHz
                        while (downsampleBuffer.length >= CHUNK_SAMPLES) {
                            const frame = downsampleBuffer.slice(0, CHUNK_SAMPLES);
                            downsampleBuffer = downsampleBuffer.slice(CHUNK_SAMPLES);
                            const bytes = floatTo16PCM(new Float32Array(frame));
                            sendAudioChunk(bytes);
                        }
                    }
                };

                sourceNode.connect(processorNode);
                processorNode.connect(audioContext.destination);
                // Try resume if autoplay policy suspended the context
                if (audioContext.state === 'suspended') {
                    audioContext.resume().catch(()=>{});
                }
                isListening = true;
                document.getElementById('talk-btn').classList.add('recording');
                document.getElementById('mic-status').textContent = 'üéß Listening continuously...';
                updateStatus('thinking', 'Listening...');
            }).catch(err => {
                console.error('[ERROR] getUserMedia error:', err);
                console.error('[ERROR] Error name:', err.name);
                console.error('[ERROR] Error message:', err.message);
                alert('È∫¶ÂÖãÈ£éËÆøÈóÆÂ§±Ë¥•: ' + err.message + '\n\nËØ∑Ê£ÄÊü•:\n1. ÊµèËßàÂô®ÊòØÂê¶ÊúâÈ∫¶ÂÖãÈ£éÊùÉÈôê\n2. Á≥ªÁªüËÆæÁΩÆ‰∏≠ÊòØÂê¶ÂÖÅËÆ∏ÊµèËßàÂô®ËÆøÈóÆÈ∫¶ÂÖãÈ£é\n3. ÊòØÂê¶ÊúâÂÖ∂‰ªñÂ∫îÁî®Âç†Áî®È∫¶ÂÖãÈ£é');
                updateStatus('disconnected', 'Mic error');
            });
        }

        function stopMicStreaming() {
            if (processorNode) {
                try { processorNode.disconnect(); } catch (e) {}
                processorNode = null;
            }
            if (sourceNode) {
                try { sourceNode.disconnect(); } catch (e) {}
                sourceNode = null;
            }
            if (audioContext) {
                try { audioContext.close(); } catch (e) {}
                audioContext = null;
            }
            if (micStream) {
                micStream.getTracks().forEach(t => t.stop());
                micStream = null;
            }
            isListening = false;
            document.getElementById('talk-btn').classList.remove('recording');
            document.getElementById('mic-status').textContent = 'Click to start/stop listening';
            updateStatus('connected', 'Ready');
        }

        function downsample(buffer, inputRate, outputRate) {
            if (outputRate === inputRate) return buffer;
            const ratio = inputRate / outputRate;
            const newLength = Math.floor(buffer.length / ratio);
            const result = new Float32Array(newLength);
            let offsetResult = 0;
            let offsetBuffer = 0;
            while (offsetResult < result.length) {
                const nextOffsetBuffer = Math.round((offsetResult + 1) * ratio);
                let accum = 0, count = 0;
                for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
                    accum += buffer[i];
                    count++;
                }
                result[offsetResult] = accum / (count || 1);
                offsetResult++;
                offsetBuffer = nextOffsetBuffer;
            }
            return result;
        }

        function floatTo16PCM(float32Array) {
            const buffer = new ArrayBuffer(float32Array.length * 2);
            const view = new DataView(buffer);
            let offset = 0;
            for (let i = 0; i < float32Array.length; i++) {
                let s = Math.max(-1, Math.min(1, float32Array[i]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
                offset += 2;
            }
            return new Uint8Array(buffer);
        }

        function sendAudioChunk(uint8Array) {
            if (!ws || ws.readyState !== WebSocket.OPEN) return;
            // base64 encode
            let binary = '';
            const len = uint8Array.length;
            for (let i = 0; i < len; i++) binary += String.fromCharCode(uint8Array[i]);
            const b64 = btoa(binary);
            ws.send(JSON.stringify({ type: 'audio_chunk', data: b64 }));
        }

        function sendTextToAI(text) {
            if (!ws || ws.readyState !== WebSocket.OPEN) {
                updateStatus('disconnected', 'Not connected');
                return;
            }

            updateStatus('thinking', 'Thinking...');
            ws.send(JSON.stringify({ type: 'user_input', text: text }));
        }

        // Ê£ÄÊµãÈ∫¶ÂÖãÈ£éÊµÅÊòØÂê¶ÂèØÁî®
        const hasMediaDevices = !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
        const hasSpeechRecognition = !!(window.SpeechRecognition || window.webkitSpeechRecognition);

        console.log('[INFO] Media Devices support:', hasMediaDevices);
        console.log('[INFO] Speech Recognition support:', hasSpeechRecognition);

        // ÊåâÈíÆ‰∫ã‰ª∂ - Toggle Ê®°ÂºèÔºàÁÇπÂáªÂºÄÂßã/ÂÅúÊ≠¢ÊåÅÁª≠ÁõëÂê¨Ôºâ
        const talkBtn = document.getElementById('talk-btn');

        talkBtn.addEventListener('click', (e) => {
            e.preventDefault();
            console.log('[DEBUG] Button clicked, isListening:', isListening);
            console.log('[DEBUG] WebSocket state:', ws ? ws.readyState : 'null');

            if (!isListening) {
                console.log('[DEBUG] Starting listening...');

                // ‰ºòÂÖà‰ΩøÁî®ÊµèËßàÂô®ËØ≠Èü≥ËØÜÂà´ÔºàÊõ¥ÁÆÄÂçïÔºåÂÖºÂÆπÊÄßÊõ¥Â•ΩÔºâ
                if (hasSpeechRecognition) {
                    console.log('[INFO] Using Web Speech API');
                    startSpeechRecognition();
                } else if (hasMediaDevices) {
                    console.log('[INFO] Using mic streaming');
                    startMicStreaming();
                } else {
                    alert('ÊÇ®ÁöÑÊµèËßàÂô®‰∏çÊîØÊåÅËØ≠Èü≥ËæìÂÖ•\n\nËØ∑Â∞ùËØï:\n1. ‰ΩøÁî® Safari Êàñ Chrome ÊµèËßàÂô®\n2. Á°Æ‰øùÈÄöËøá http://localhost:8000 ËÆøÈóÆ\n3. ÊàñËÄÖÁõ¥Êé•ËæìÂÖ•ÊñáÂ≠ó‰∏é Ani ÂØπËØù');
                    return;
                }

                talkBtn.textContent = 'üî¥ Stop Listening';
                talkBtn.style.background = 'linear-gradient(135deg, #ef4444, #dc2626)';
            } else {
                console.log('[DEBUG] Stopping listening...');

                if (recognition) {
                    stopSpeechRecognition();
                } else {
                    stopMicStreaming();
                }

                talkBtn.textContent = 'üé§ Start Listening';
                talkBtn.style.background = 'linear-gradient(135deg, #667eea, #764ba2)';
            }
        });

        // Èü≥È¢ëÊøÄÊ¥ªÊ†áÂøó
        let audioActivated = false;

        // ÊøÄÊ¥ªÈü≥È¢ëÊí≠ÊîæÔºàËß£ÂÜ≥ÊµèËßàÂô®Ëá™Âä®Êí≠ÊîæÈôêÂà∂Ôºâ
        function activateAudio() {
            if (audioActivated) return;

            console.log('[AUDIO] Activating audio context...');

            // Êí≠Êîæ‰∏Ä‰∏™ÈùôÈü≥Èü≥È¢ëÊù•ÊøÄÊ¥ª
            const silentAudio = new Audio();
            silentAudio.src = 'data:audio/wav;base64,UklGRigAAABXQVZFZm10IBIAAAABAAEARKwAAIhYAQACABAAAABkYXRhAgAAAAEA';
            silentAudio.volume = 0;
            silentAudio.play().then(() => {
                console.log('[OK] Audio context activated');
                audioActivated = true;
            }).catch(() => {
                console.log('[INFO] Audio activation will retry on next interaction');
            });

            // ÊÅ¢Â§ç AudioContextÔºàÂ¶ÇÊûúÂ≠òÂú®Ôºâ
            try {
                if (audioContext && audioContext.state === 'suspended') {
                    audioContext.resume();
                }
            } catch(e) {}
        }

        // Âú®ÊâÄÊúâÁî®Êà∑‰∫§‰∫í‰∫ã‰ª∂‰∏äÂ∞ùËØïÊøÄÊ¥ªÈü≥È¢ë
        ['click', 'keydown', 'touchstart', 'mousedown'].forEach(evt => {
            window.addEventListener(evt, activateAudio, { passive: true, once: false });
        });

        // È°µÈù¢Âä†ËΩΩÊó∂ÊèêÁ§∫Áî®Êà∑‰∫§‰∫í
        window.addEventListener('load', () => {
            console.log('[INFO] üí° ÊèêÁ§∫: ÁÇπÂáªÈ°µÈù¢‰ªªÊÑè‰ΩçÁΩÆ‰ª•ÊøÄÊ¥ªÈü≥È¢ëÊí≠Êîæ');
        });

        // ===== Natural Idle + Micro Gestures =====
        // Runtime state
        let aniState = 'connected'; // externally updated via WS 'state'
        let idleEnabled = true;
        let microGestureEnabled = true;

        // Idle config (base subtle motion)
        const idleCfg = {
            chest: { amp: 0.02, freq: 1.1 }, // radians, Hz-ish
            head: { ampYaw: 0.05, ampPitch: 0.03, freqYaw: 0.35, freqPitch: 0.28 },
            arm: { amp: 0.05, freq: 0.8 }
        };

        // Jitter offsets to reduce periodic feeling
        let jitter = { headYaw: 0, headPitch: 0 };
        let jitterT = 0;

        function stepJitter(delta) {
            jitterT += delta;
            // small random walk bounded in ¬±0.01 rad
            const step = 0.0025;
            jitter.headYaw += (Math.random() - 0.5) * step;
            jitter.headPitch += (Math.random() - 0.5) * step;
            jitter.headYaw = Math.max(-0.01, Math.min(0.01, jitter.headYaw));
            jitter.headPitch = Math.max(-0.01, Math.min(0.01, jitter.headPitch));
        }

        // Micro gesture scheduler
        let micro = { active: false, type: null, t: 0, dur: 1.0, cooldownT: 0, cooldown: 8.0 + Math.random() * 7.0 };
        
        // Capture base rotations to avoid drift (absolute offsets per frame)
        const baseRot = { captured: false, chestX: 0, headX: 0, headY: 0, headZ: 0, leftArm: [0,0,0], rightArm: [0,0,0] };
        function captureBaseRot() {
            if (!vrm || !vrm.humanoid || baseRot.captured) return;
            const chest = vrm.humanoid.getNormalizedBoneNode('chest');
            const head = vrm.humanoid.getNormalizedBoneNode('head');
            const leftArm = vrm.humanoid.getNormalizedBoneNode('leftUpperArm');
            const rightArm = vrm.humanoid.getNormalizedBoneNode('rightUpperArm');
            if (chest) baseRot.chestX = chest.rotation.x;
            if (head) {
                baseRot.headX = head.rotation.x;
                baseRot.headY = head.rotation.y;
                baseRot.headZ = head.rotation.z;
            }
            if (leftArm) baseRot.leftArm = [leftArm.rotation.x, leftArm.rotation.y, leftArm.rotation.z];
            if (rightArm) baseRot.rightArm = [rightArm.rotation.x, rightArm.rotation.y, rightArm.rotation.z];
            baseRot.captured = true;
        }

        function maybeStartMicroGesture() {
            if (!microGestureEnabled || micro.active) return;
            if (aniState === 'speaking' || currentGesture) return;
            if (micro.cooldownT < micro.cooldown) return;
            // 20% chance when cooldown reached
            if (Math.random() < 0.2) {
                const types = ['headTilt', 'nodSmall', 'handWaveSmall', 'lookAside'];
                micro.type = types[Math.floor(Math.random() * types.length)];
                micro.dur = 0.6 + Math.random() * 0.6;
                micro.t = 0;
                micro.active = true;
                micro.cooldownT = 0;
                micro.cooldown = 8.0 + Math.random() * 7.0;
            }
        }

        function updateMicroGesture(delta) {
            // advance cooldown
            micro.cooldownT += delta;
            if (!micro.active) {
                maybeStartMicroGesture();
                return;
            }
            micro.t += delta;
            const p = Math.min(1.0, micro.t / micro.dur);
            const e = Math.sin(p * Math.PI); // ease in-out

            if (vrm && vrm.humanoid) {
                const head = vrm.humanoid.getNormalizedBoneNode('head');
                const rightArm = vrm.humanoid.getNormalizedBoneNode('rightUpperArm');
                switch (micro.type) {
                    case 'headTilt':
                        if (head && !currentGesture) head.rotation.z = baseRot.headZ + e * 0.15; // ¬±8.5¬∞
                        break;
                    case 'nodSmall':
                        if (head && !currentGesture) head.rotation.x = baseRot.headX + Math.sin(p * Math.PI * 2.0) * 0.12; // Â∞èÂπÖÁÇπÂ§¥
                        break;
                    case 'handWaveSmall':
                        if (rightArm && !currentGesture) rightArm.rotation.z = baseRot.rightArm[2] - Math.sin(p * Math.PI * 3.0) * 0.25;
                        break;
                    case 'lookAside':
                        if (head && !currentGesture) head.rotation.y = baseRot.headY + Math.sin(p * Math.PI) * 0.18; // Â∑¶Âè≥Áúã
                        break;
                }
            }

            if (micro.t >= micro.dur) {
                // end and let idle/pose restore
                micro.active = false;
                micro.type = null;
                micro.t = 0;
            }
        }

        // Âä®ÁîªÂæ™ÁéØ
        function animate() {
            requestAnimationFrame(animate);

            const delta = clock.getDelta();
            time += delta;

            if (vrm) {
                vrm.update(delta);
                updateExpression(delta);
                updateBlink(delta);
                updateGesture(delta);
                // Idle + micro gestures
                stepJitter(delta);
                updateMicroGesture(delta);

                if (vrm.humanoid) {
                    const chest = vrm.humanoid.getNormalizedBoneNode('chest');
                    if (chest && !currentGesture) {
                        const f = idleCfg.chest.freq;
                        chest.rotation.x = baseRot.chestX + Math.sin(time * f) * idleCfg.chest.amp;
                    }

                    const head = vrm.humanoid.getNormalizedBoneNode('head');
                    if (head && !currentGesture && !micro.active) {
                        head.rotation.y = baseRot.headY + Math.sin(time * idleCfg.head.freqYaw) * idleCfg.head.ampYaw + jitter.headYaw;
                        head.rotation.x = baseRot.headX + Math.cos(time * idleCfg.head.freqPitch) * idleCfg.head.ampPitch + jitter.headPitch;
                    }

                    if (!currentGesture) {
                        const leftArm = vrm.humanoid.getNormalizedBoneNode('leftUpperArm');
                        const rightArm = vrm.humanoid.getNormalizedBoneNode('rightUpperArm');

                        if (leftArm) {
                            leftArm.rotation.set(
                                baseRot.leftArm[0] + Math.sin(time * idleCfg.arm.freq) * idleCfg.arm.amp,
                                baseRot.leftArm[1],
                                baseRot.leftArm[2] + Math.sin(time * 0.6) * 0.03
                            );
                        }

                        if (rightArm) {
                            rightArm.rotation.set(
                                baseRot.rightArm[0] + Math.sin(time * idleCfg.arm.freq + Math.PI) * idleCfg.arm.amp,
                                baseRot.rightArm[1],
                                baseRot.rightArm[2] - Math.sin(time * 0.6) * 0.03
                            );
                        }
                    }
                }
            }

            controls.update();
            renderer.render(scene, camera);
        }

        // ÂêØÂä®
        window.addEventListener('load', () => {
            connectWebSocket();
            animate();
        });

        window.ani = {
            setExpression,
            sendText: sendTextToAI,
            vrm: () => vrm
        };
    </script>
</body>
</html>
