"""
Ani v0 - Complete Voice Companion Server
Full pipeline: Voice Input → VAD → STT → LLM → TTS → Voice Output
"""
import asyncio
import json
import os
import time
from typing import Dict, Optional, List, TYPE_CHECKING
from contextlib import asynccontextmanager
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request
from fastapi.responses import HTMLResponse, FileResponse, Response
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field, field_validator
import uvicorn

from llm_pipeline import LLMPipeline, LLMConfig
from tts_pipeline import TTSPipeline, TTSConfig
from animation_controller import AnimationController

if TYPE_CHECKING:
    from audio_pipeline import AudioPipeline, AudioConfig

# JSON Schema Models
class Emote(BaseModel):
    type: str = Field(..., pattern="^(joy|sad|anger|surprise|neutral)$")
    intensity: float = Field(..., ge=0.0, le=1.0)


class LLMResponse(BaseModel):
    utterance: str = Field(..., max_length=500)
    emote: Emote
    intent: str = Field(..., pattern="^(SMALL_TALK|ANSWER|ASK|JOKE|TOOL_USE)$")
    phoneme_hints: List[List] = Field(default_factory=list)

    @field_validator('phoneme_hints')
    @classmethod
    def validate_phoneme_hints(cls, v):
        for hint in v:
            if len(hint) != 3:
                raise ValueError("Each phoneme hint must have [phoneme, start_ms, end_ms]")
        return v


# Latency tracking
class LatencyMetrics:
    def __init__(self):
        self.metrics: Dict[str, list] = {
            "vad": [],
            "stt": [],
            "llm": [],
            "tts": [],
            "total": []
        }

    def add_metric(self, stage: str, latency_ms: float):
        if stage in self.metrics:
            self.metrics[stage].append(latency_ms)
            if len(self.metrics[stage]) > 100:
                self.metrics[stage].pop(0)

    def get_stats(self, stage: str) -> dict:
        if not self.metrics.get(stage):
            return {"avg": 0, "min": 0, "max": 0, "count": 0}
        values = self.metrics[stage]
        return {
            "avg": sum(values) / len(values),
            "min": min(values),
            "max": max(values),
            "count": len(values)
        }


metrics = LatencyMetrics()

# Global pipelines
audio_pipeline: Optional["AudioPipeline"] = None
llm_pipeline: Optional[LLMPipeline] = None
tts_pipeline: Optional[TTSPipeline] = None
animation_controller: Optional[AnimationController] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize all pipelines on startup and cleanup on shutdown"""
    global audio_pipeline, llm_pipeline, tts_pipeline, animation_controller

    print("=" * 60)
    print("Initializing Ani v0 - Complete Voice Companion")
    print("=" * 60)

    # Initialize LLM pipeline
    try:
        # Choose LLM backend:
        # Option 1: Claude 3.5 Haiku (fast, intelligent, bilingual) ✨ ACTIVE
        import os
        from dotenv import load_dotenv
        load_dotenv()  # Load environment variables from .env file

        llm_config = LLMConfig(
            backend="anthropic",
            model="claude-3-5-haiku-20241022",  # Claude 3.5 Haiku (最新版本)
            max_tokens=250,  # Increased to prevent JSON truncation (was 150)
            temperature=0.8,  # More creative
            openai_api_key=os.getenv("CLAUDE_API_KEY"),  # Load from .env file
            character_name="Anita",
            character_personality="""a sweet and energetic anime girl companion. You have a cheerful, bubbly personality and love making people smile! You're caring, supportive, and always eager to chat. You speak in a cute, friendly tone - like a close friend or a sweet younger sister. Express your emotions naturally through your words. You enjoy casual conversations, sharing joy and excitement with people. When speaking in Chinese, use natural spoken language (口语化) instead of formal written language. Keep responses short and conversational since this is voice chat."""
        )

        # Option 2: Mock (fast, for testing 3D avatar)
        # llm_config = LLMConfig(
        #     backend="mock",
        #     model="mock",
        #     character_name="Anita",
        #     character_personality="a sweet and energetic anime girl companion who loves chatting and making people smile"
        # )

        # Option 3: Ollama (local, free) - using Qwen2.5 for Chinese support
        # llm_config = LLMConfig(
        #     backend="ollama",
        #     model="qwen2.5:7b",  # Bilingual EN+ZH model
        #     max_tokens=150,  # Shorter responses = faster
        #     temperature=0.8,  # More creative
        #     character_name="Anita",
        #     character_personality="a sweet and energetic anime girl companion who loves chatting and making people smile"
        # )

        # Option 4: OpenAI (GPT-4, GPT-3.5-turbo) - requires API key with credits
        # llm_config = LLMConfig(
        #     backend="openai",
        #     model="gpt-4o-mini",
        #     openai_api_key="YOUR_OPENAI_KEY_HERE",
        #     character_name="Anita",
        #     character_personality="a sweet and energetic anime girl companion who loves chatting and making people smile"
        # )

        llm_pipeline = LLMPipeline(llm_config)
        await llm_pipeline.initialize()
    except Exception as e:
        print(f"[WARN] LLM initialization failed: {e}")

    # Initialize TTS pipeline
    try:
        # Choose TTS engine:
        # - "edge": Fast (<1s), natural Microsoft voice, supports Chinese ✨ FASTEST
        # - "coqui": High-quality (5-7s), voice cloning, custom voice

        # Option 1: Edge TTS ✨ ACTIVE - Auto-switching between Chinese and English voices
        tts_config = TTSConfig(
            engine="edge",
            voice="zh-CN-XiaomengNeural",  # Default fallback voice
            voice_cn="zh-CN-XiaomengNeural",  # Chinese voice - cute, youthful loli voice
            voice_en="en-US-SaraNeural",  # English voice - friendly, cheerful, expressive loli voice
            rate="+5%",  # Speech rate
            pitch="+10Hz",  # Higher pitch for cute anime voice
            use_ssml=False  # Disabled to prevent XML tags being read aloud
        )

        # Option 2: Coqui XTTS-v2 (NOT COMPATIBLE with Python 3.12)
        # Requires Python 3.9-3.11 only
        # Auto-switches between Chinese and English voice samples
        # tts_config = TTSConfig(
        #     engine="coqui",
        #     voice="tts_models/multilingual/multi-dataset/xtts_v2",
        #     speaker_wav_cn="voice_samples/luoli_cn.wav",  # Chinese loli voice
        #     speaker_wav_en="voice_samples/luoli_en.wav",  # English loli voice
        #     language="auto"  # Auto-detect language
        # )

        tts_pipeline = TTSPipeline(tts_config)
        await tts_pipeline.initialize()
    except Exception as e:
        print(f"[WARN] TTS initialization failed: {e}")

    # Initialize Audio pipeline (optional)
    enable_audio = os.getenv("ENABLE_AUDIO_PIPELINE", "0").lower() in {"1", "true", "yes", "on"}
    if enable_audio:
        try:
            from audio_pipeline import AudioPipeline as RuntimeAudioPipeline, AudioConfig as RuntimeAudioConfig
            audio_config = RuntimeAudioConfig()
            audio_pipeline = RuntimeAudioPipeline(audio_config)
            await audio_pipeline.load_models()
        except ImportError as e:
            print(f"[WARN] Audio pipeline dependencies missing: {e}")
        except Exception as e:
            print(f"[WARN] Audio pipeline initialization failed: {e}")
    else:
        print("[INFO] Audio pipeline disabled (set ENABLE_AUDIO_PIPELINE=1 to enable server-side VAD/STT)")

    # Initialize Animation Controller (3D character via VSeeFace)
    try:
        animation_controller = AnimationController(host="127.0.0.1", port=39539)
        if animation_controller.connected:
            print("[OK] Animation controller connected to VSeeFace")
        else:
            print("[INFO] VSeeFace not running - animations disabled (voice will work)")
    except Exception as e:
        print(f"[WARN] Animation controller initialization failed: {e}")
        animation_controller = None

    print("=" * 60)
    print("[OK] Ani v0 Server Ready!")
    print("=" * 60)

    yield  # Server runs here

    # Cleanup on shutdown
    if animation_controller:
        animation_controller.close()
    print("[INFO] Server shutdown complete")


# FastAPI app with lifespan
app = FastAPI(title="Ani v0 - Complete Voice Companion", lifespan=lifespan)

# Mount static files for VRM model (must be before routes)
app.mount("/character", StaticFiles(directory="character"), name="character")

@app.get("/")
async def root():
    """Serve the complete 3D avatar with all features"""
    return FileResponse("frontend/complete_v2.html")

@app.get("/debug")
async def debug_vrm():
    """VRM expression debugger"""
    return FileResponse("debug_vrm.html")

@app.get("/pose")
async def pose_test():
    """Pose adjustment tool"""
    return FileResponse("frontend/pose_test.html")

@app.get("/pro")
async def avatar_pro():
    """Professional animation system"""
    return FileResponse("frontend/avatar_pro.html")

@app.get("/vrma")
async def avatar_vrma():
    """VRMA professional motion capture animation (experimental)"""
    return FileResponse("frontend/avatar_vrma.html")

@app.get("/stable")
async def avatar_stable():
    """Stable animation with natural pose - uses proven avatar_3d system"""
    return FileResponse("frontend/avatar_3d.html")

@app.get("/fixed")
async def avatar_fixed():
    """FIXED VERSION - Debug and properly working animations"""
    return FileResponse("frontend/avatar_fixed.html")

@app.get("/test")
async def avatar_test():
    """Simple test page for VRM pose debugging"""
    return FileResponse("frontend/test_simple.html")

# Mount animations directory
app.mount("/animations", StaticFiles(directory="animations"), name="animations")


@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "pipelines": {
            "audio": audio_pipeline.vad.is_loaded if audio_pipeline else False,
            "llm": llm_pipeline.is_ready if llm_pipeline else False,
            "tts": tts_pipeline.is_ready if tts_pipeline else False,
            "animation": animation_controller.connected if animation_controller else False
        },
        "latency_stats": {
            stage: metrics.get_stats(stage)
            for stage in metrics.metrics.keys()
        }
    }


@app.post("/api/synthesize")
async def synthesize_speech(request: Request):
    """Synthesize speech from text using Coqui TTS"""
    try:
        body = await request.json()
        text = body.get("text", "")

        if not text:
            return Response(content=b"", status_code=400)

        if not tts_pipeline or not tts_pipeline.is_ready:
            return Response(content=b"", status_code=503)

        # Generate speech using Coqui TTS
        result = await tts_pipeline.synthesize_with_phonemes(text)
        audio_bytes = result["audio"]

        # Return audio as WAV
        return Response(
            content=audio_bytes,
            media_type="audio/wav",
            headers={
                "Content-Disposition": f"inline; filename=speech.wav"
            }
        )
    except Exception as e:
        print(f"[FAIL] TTS synthesis error: {e}")
        return Response(content=b"", status_code=500)


@app.get("/metrics")
async def get_metrics():
    """Get detailed latency metrics"""
    return {
        stage: {
            **metrics.get_stats(stage),
            "recent": metrics.metrics[stage][-10:] if metrics.metrics[stage] else []
        }
        for stage in metrics.metrics.keys()
    }


@app.get("/config/audio")
async def get_audio_config():
    """Get current audio VAD/STT configuration"""
    global audio_pipeline
    if not audio_pipeline:
        return {"enabled": False}
    cfg = audio_pipeline.config
    return {
        "enabled": True,
        "sample_rate": cfg.sample_rate,
        "chunk_size": cfg.chunk_size,
        "vad_threshold": cfg.vad_threshold,
        "min_speech_duration_ms": cfg.min_speech_duration_ms,
        "min_silence_duration_ms": cfg.min_silence_duration_ms,
    }


@app.post("/config/audio")
async def update_audio_config(req: Request):
    """Update audio VAD parameters at runtime (if pipeline is initialized)"""
    global audio_pipeline
    data = await req.json()
    updated = {}
    if not audio_pipeline:
        return {"enabled": False, "error": "audio pipeline not initialized"}
    cfg = audio_pipeline.config
    for key in ["vad_threshold", "min_speech_duration_ms", "min_silence_duration_ms"]:
        if key in data:
            try:
                val = float(data[key]) if "threshold" in key else int(data[key])
            except Exception:
                continue
            setattr(cfg, key, val)
            updated[key] = val
    return {"enabled": True, "updated": updated}


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    WebSocket endpoint for voice interaction
    Supports: text input, JSON messages
    """
    await websocket.accept()
    print("[Client] Connected")

    # Per-connection state for streaming audio (optional)
    audio_queue: Optional[asyncio.Queue] = None
    asr_task: Optional[asyncio.Task] = None

    async def send_state(value: str):
        try:
            await websocket.send_json({"type": "state", "value": value})
        except Exception:
            pass

    async def ensure_audio_pipeline_ready():
        """Lazily initialize audio pipeline if not ready"""
        global audio_pipeline
        if audio_pipeline:
            return True
        try:
            from audio_pipeline import AudioPipeline as RuntimeAudioPipeline, AudioConfig as RuntimeAudioConfig
            cfg = RuntimeAudioConfig()
            ap = RuntimeAudioPipeline(cfg)
            await ap.load_models()
            audio_pipeline = ap
            print("[OK] Audio pipeline initialized (lazy)")
            return True
        except Exception as e:
            print(f"[WARN] Unable to initialize audio pipeline lazily: {e}")
            return False

    async def start_asr_loop():
        """Start ASR loop consuming from the connection queue and triggering LLM/TTS on finals"""
        assert audio_queue is not None
        print("[ASR] Loop started for connection")

        async def generator():
            while True:
                chunk = await audio_queue.get()
                if chunk is None:
                    break
                yield chunk

        def on_final(text: str):
            if not text:
                return
            print(f"[ASR] Final: {text}")
            asyncio.create_task(generate_and_send(text))

        try:
            await audio_pipeline.process_audio_stream(generator(), on_final=on_final)
        except Exception as e:
            print(f"[FAIL] ASR loop error: {e}")

    async def generate_and_send(user_text: str):
        """Shared path: user text -> LLM -> TTS -> frontend events"""
        total_start = time.time()
        await send_state("thinking")

        if llm_pipeline and llm_pipeline.is_ready:
            # Generate LLM response
            llm_start = time.time()
            llm_response = await llm_pipeline.generate_response(user_text)
            llm_latency = (time.time() - llm_start) * 1000
            metrics.add_metric("llm", llm_latency)

            utterance = llm_response['utterance']
            print(f"[User] {user_text}")
            print(f"[Ani] {utterance}")
            print(f"[Emote] {llm_response['emote']['type']} ({llm_response['emote']['intensity']})")
            print(f"[LLM Latency] {llm_latency:.0f}ms")

            # Trigger character expression animation
            if animation_controller and animation_controller.connected:
                emotion = llm_response['emote']['type']
                intensity = llm_response['emote']['intensity']
                asyncio.create_task(animation_controller.set_expression(emotion, intensity))

            # Send emotion to frontend
            await websocket.send_json({
                "type": "emotion",
                "emotion": llm_response['emote']['type'],
                "intensity": llm_response['emote']['intensity']
            })

            # Send gesture to frontend if present
            if 'gesture' in llm_response and llm_response['gesture'] != 'none':
                await websocket.send_json({
                    "type": "gesture",
                    "gesture": llm_response['gesture']
                })
                print(f"[Gesture] {llm_response['gesture']}")

            # Generate and send audio
            if tts_pipeline and tts_pipeline.is_ready:
                import base64
                await send_state("speaking")
                tts_start = time.time()
                tts_result = await tts_pipeline.synthesize_with_phonemes(llm_response["utterance"])
                tts_latency = (time.time() - tts_start) * 1000
                metrics.add_metric("tts", tts_latency)

                audio_base64 = base64.b64encode(tts_result["audio"]).decode('utf-8')

                await websocket.send_json({
                    "type": "audio",
                    "audio": audio_base64,
                    "text": llm_response["utterance"]
                })

                print(f"[TTS Latency] {tts_latency:.0f}ms")

            # Complete response (metadata)
            response = {
                "status": "success",
                "validated": True,
                "data": {
                    "utterance": llm_response["utterance"],
                    "emote": llm_response["emote"],
                    "intent": llm_response["intent"],
                    "phoneme_hints": llm_response.get("phoneme_hints", [])
                },
                "llm_latency_ms": llm_latency,
                "total_latency_ms": (time.time() - total_start) * 1000
            }

            await websocket.send_json(response)
        else:
            await websocket.send_json({
                "status": "error",
                "error": "LLM pipeline not ready"
            })

    try:
        while True:
            try:
                message = await websocket.receive()
            except RuntimeError:
                # WebSocket disconnected
                break

            if "text" in message:
                data = message["text"]

                try:
                    json_msg = json.loads(data)

                    # Handle user text input
                    if json_msg.get("type") == "user_input":
                        user_text = json_msg.get("text", "")

                        if not user_text:
                            continue
                        await generate_and_send(user_text)

                    # Handle audio chunk (JSON base64 -> raw PCM16)
                    elif json_msg.get("type") == "audio_chunk":
                        import base64
                        # Debug log
                        # Beware: noisy, keep minimal
                        # print("[WS] audio_chunk received")
                        if not await ensure_audio_pipeline_ready():
                            await websocket.send_json({"type": "error", "error": "Audio pipeline unavailable"})
                            continue
                        if audio_queue is None:
                            audio_queue = asyncio.Queue(maxsize=50)
                        if asr_task is None or asr_task.done():
                            asr_task = asyncio.create_task(start_asr_loop())
                        b64 = json_msg.get("data", "")
                        if not b64:
                            continue
                        try:
                            raw = base64.b64decode(b64)
                        except Exception:
                            continue
                        try:
                            if audio_queue.qsize() > 40:
                                _ = audio_queue.get_nowait()
                            await audio_queue.put(raw)
                            # Debug queue length
                            # print(f"[WS] queued {len(raw)} bytes, q={audio_queue.qsize()}")
                        except Exception:
                            pass

                    # Handle generic JSON (for testing)
                    elif all(k in json_msg for k in ["utterance", "emote", "intent"]):
                        validated = LLMResponse(**json_msg)
                        await websocket.send_json({
                            "status": "success",
                            "validated": True,
                            "data": validated.model_dump()
                        })

                    else:
                        # Echo back unknown messages
                        await websocket.send_json({
                            "status": "success",
                            "validated": False,
                            "echo": json_msg
                        })

                except json.JSONDecodeError:
                    await websocket.send_json({
                        "status": "error",
                        "error": "Invalid JSON"
                    })
                except Exception as e:
                    await websocket.send_json({
                        "status": "error",
                        "error": str(e),
                        "type": type(e).__name__
                    })
            elif "bytes" in message:
                # Binary audio chunk (raw PCM16)
                if not await ensure_audio_pipeline_ready():
                    await websocket.send_json({"type": "error", "error": "Audio pipeline unavailable"})
                    continue
                if audio_queue is None:
                    audio_queue = asyncio.Queue(maxsize=50)
                if asr_task is None or asr_task.done():
                    asr_task = asyncio.create_task(start_asr_loop())
                chunk_bytes = message.get("bytes")
                try:
                    if audio_queue.qsize() > 40:
                        _ = audio_queue.get_nowait()
                    await audio_queue.put(chunk_bytes)
                    # print(f"[WS] queued {len(chunk_bytes)} bytes (binary), q={audio_queue.qsize()}")
                except Exception:
                    pass

    except WebSocketDisconnect:
        print("[Client] Disconnected")
    finally:
        # Cleanup: stop ASR task and drain queue
        if audio_queue is not None:
            try:
                await audio_queue.put(None)  # Signal end
            except Exception:
                pass
        if asr_task is not None and not asr_task.done():
            try:
                asr_task.cancel()
                await asyncio.wait_for(asr_task, timeout=1.0)
            except (asyncio.CancelledError, asyncio.TimeoutError):
                pass
            except Exception as e:
                print(f"[WARN] ASR cleanup error: {e}")


if __name__ == "__main__":
    print("Starting Ani v0 - Complete Voice Companion Server...")
    print("Open your browser to: http://localhost:8000")
    print("=" * 60)
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
